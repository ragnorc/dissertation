
% to choose your degree
% please un-comment just one of the following

\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}     % for BSc, BEng etc.
% \documentclass[minf,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}  % for MInf

\usepackage{bm}
\usepackage[thinc]{esdiff}
\usepackage{todonotes}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{placeins}

\let\Oldsection\section
\renewcommand{\section}{\FloatBarrier\Oldsection}

\let\Oldsubsection\subsection
\renewcommand{\subsection}{\FloatBarrier\Oldsubsection}

\let\Oldsubsubsection\subsubsection
\renewcommand{\subsubsection}{\FloatBarrier\Oldsubsubsection}

%\reversemarginpar
\begin{document}

\title{An Interpretable Deep Learning Approach to Pharmacodynamic Mechanism of Action Prediction}

\author{Ragnor Comerford}

% to choose your course
% please un-comment just one of the following
\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Software Engineering}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Psychology }   
%\course{Artificial Intelligence with Psychology }   
%\course{Linguistics and Artificial Intelligence}    
%\course{Computer Science}
%\course{Software Engineering}
%\course{Computer Science and Electronics}    
%\course{Electronics and Software Engineering}    
%\course{Computer Science and Management Science}    
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}  
%\course{Computer Science and Statistics}    

% to choose your report type
% please un-comment just one of the following
\project{Undergraduate Dissertation} % CS&E, E&SE, AI&L
%\project{Undergraduate Thesis} % AI%Psy
%\project{4th Year Project Report}

\date{\today}

\abstract{
This is an example of {\tt infthesis} style.
The file {\tt skeleton.tex} generates this document and can be 
used to get a ``skeleton'' for your thesis.
The abstract should summarise your report and fit in the space on the 
first page.
%
You may, of course, use any other software to write your report,
as long as you follow the same style. That means: producing a title
page as given here, and including a table of contents and bibliography.
}

\maketitle

\section*{Acknowledgements}
I would like to express my sincere gratitude to my supervisor, Michael Herrmann, for his valuable guidance and insightful feedback. I would also like to thank my family and friends for their constant support and love
throughout my academic years.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}

\section{Motivation}
Drug discovery has traditionally been an expensive and inefficient process with a very serendipitous approach to the identification of novel drug compounds.
Despite the huge scientific and technological advances such as high-throughput screening (HTS), parallel synthesis and combinatorial chemistry in the past decade, the biopharmaceutical industry has not seen much increase in efficiency. This development, coined \textit{Eroom's Law}, states that drug discovery has become slower and more expensive with a steadily decrease in efficiency since the 1980s \cite{scannell_diagnosing_2012}. But it is as much an issue of science as it is a social and political one. The cost of each new molecular entity (NME) has climbed to approximately US\$1.8 billion \cite{paul_how_2010}. as "the number of new drugs approved per billion US dollars spent on R\&D has halved roughly every 9 years since 1950, falling around 80-fold in inflation-adjusted terms"\cite{scannell_diagnosing_2012}.
The astronomical costs of drug development are reflected in the inequality of access to health care and disproportionately affects developing countries where a big proportion of the population is deprived of life-saving treatment. \\
However, recent advances in machine learning (ML) coupled with a significant increase in computing power and data are inducing a renaissance in drug development.
Deep learning (DL) \cite{lecun_deep_2015} with its ability to model complex non-linear relationships has successfully taken advantage of the exponential growth in the availability of data and computer hardware. The algorithmic optimization of DL algorithms for graphical processing units (GPUs) has significantly reduced the time to train DL models by leveraging the power of GPUs to significantly speed up parallel processing and matrix operations.
The explosive growth of available biomedical and compound activity databases \cite{papadatos_activity_2015}\cite{kim_pubchem_2016}\cite{gaulton_chembl_2012}, partly owed to the emergence of new experimental techniques such as HTS or parallel synthesis \cite{grada_next-generation_2013}, allows DL to perform automatic feature detection and complex representation learning from massive amounts of unlabelled or labelled biomedical data.\\
One such instance of large-scale biomedical data collection enabled by new experimental techniques is the \textit{Connectivity Map} (CMap) \cite{lamb_connectivity_2006} which plays a central role in our work. The CMap is a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the  NIH  Common  Funds  Library  of  Integrated  Network-Based  Cellular  Signatures(LINCS) that uses gene expression signatures to discover functional connections among diseases, genetic perturbation, and drug action \cite{musa_review_2017}. They developed the L1000 assay to facilitate rapid, flexible and high-throughput gene expression profiling at lower costs \cite{subramanian_next_2017} and built a comprehensive, large-scale drug perturbation database containing gene expression signatures of a wide range of cultivated cell lines treated with thousands of chemical compounds. In this work, we leverage the recent availability of such data as well as deep learning techniques to understand the relationship between drugs, genes and pathways and build an accurate model predicting the function of unknown compounds.


\section{Contributions}
\begin{itemize}
\item Literature review on existing approaches to drug action prediction 
\item Exploratory data analysis and PCA
\item Design and implementation of neural networks for mechanism of action prediction
\item Implementation of Bayesian Optimization
\item Implementation of integrated gradients method and biological enrichment for model interpretation.
\item Interpretation of model outputs in the biological and pharmacological context.
\end{itemize}
\section{Outline of the report}

\chapter{Background}
In this chapter, we introduce the reader to the biological concepts and mechanisms that are relevant to the understanding of deep learning applications to drug discovery. This is followed by a brief introduction to deep learning algorithms. \\
We assume that the reader is familiar with the basic concepts of biology and the fundamentals of linear algebra, probability, and calculus. We refer the interested reader to Goodfellow \textit{et al.} (2016) \cite{goodfellow_deep_2016} and Alberts \textit{et al.} (2013) \cite{alberts_essential_2013} for comprehensive introductions.
\section{Molecular biology}
\subsection{Gene expression}
\textit{Gene expression} forms the fundamental part of the central dogma of microbiology \cite{crick_protein_1958} and refers to the process by which the genetic information stored in the DNA is used in the synthesis of a gene product such as RNA or protein. Gene expression can be regulated by a wide range of biological mechanisms in the cell. Activation or inactivation of signaling pathways in regular physiological processes or in response to stimuli affects gene expression via transcription factors and their regulatory genes \cite{itadani_can_2008}. This cascade of signal transduction generates characteristic patterns of gene expression, commonly referred to as \textit{gene expression signatures}.
Changes in gene expression as a consequence of disease or pharmacologic perturbation (termed \textit{differential gene expression signatures}) bears significant potential in improving our understanding of disease treatment and the identification of drug targets.
\subsection{Cell viability}
\subsection{Mechanism of Action}
The \textit{mechanism of action} (MoA) of a compound refers to the set of target and effector proteins that are associated with a certain biological activity in a specific cellular context. Its correct identification for novel compounds represents a major challenge in drug development as most candidate compounds fail in the pharmacological pipeline due to toxicity associated with off-target affects and lack of efficacy \cite{wehling_assessing_2009}.

\todo[inline]{Rewrite the following:}
In the case of anti-infective drug development, the information permits anticipation of problems relating to clinical safety. Drugs disrupting the cytoplasmic membrane or electron transport chain, for example, are more likely to cause toxicity problems than those targeting components of the cell wall (peptidoglycan or Beta-glucans) or 70S ribosome, structures which are absent in human cells.[4][5]
By knowing the interaction between a certain site of a drug and a receptor, other drugs can be formulated in a way that replicates this interaction, thus producing the same therapeutic effects. Indeed, this method is used to create new drugs.
It can help identify which patients are most likely to respond to treatment. Because the breast cancer medication trastuzumab is known to target protein HER2, for example, tumors can be screened for the presence of this molecule to determine whether or not the patient will benefit from trastuzumab therapy.[6][7]
It can enable better dosing because the drug's effects on the target pathway can be monitored in the patient. Statin dosage, for example, is usually determined by measuring the patient's blood cholesterol levels.[6]
It allows drugs to be combined in such a way that the likelihood of drug resistance emerging is reduced. By knowing what cellular structure an anti-infective or anticancer drug acts upon, it is possible to administer a cocktail that inhibits multiple targets simultaneously, thereby reducing the risk that a single mutation in microbial or tumor DNA will lead to drug resistance and treatment failure.[4][8][9][10]
It may allow other indications for the drug to be identified. Discovery that sildenafil inhibits phosphodiesterase-5 (PDE-5) proteins, for example, enabled this drug to be repurposed for pulmonary arterial hypertension treatment, since PDE-5 is expressed in pulmonary hypertensive lungs.[11][12]

\subsection{CMAP}
\section{Deep learning}
\textit{Deep learning} can generally be described as a class of machine learning algorithms that uses \textit{artificial neural networks} (ANNs) with multiple layers between the input and output layers for learning data representations. ANNs are loosely inspired by the biological brain where inter-connected neurons transmit signals to each other.
The universal approximation theorem represents an important discovery in the theory of deep learning. It states that, for any continuous function \(f\) on a compact space \(K\), there exists a single-hidden-layer feedforward neural network which uniformly approximates \(f\) within an arbitrary error \(\epsilon>0\) on \(K\) \cite{hornik_multilayer_1989}.

Why is deep learning helpful in drug discovery (features
Limitations
Recent advances
\subsection{Feedforward neural network}
A \textit{feedforward neural network} consists of an input layer and output layer that passes information in a forward fashion through a series of latent layers with the ultimate goal of approximating a function with a composition of intermediate computations. More formally, a feedfoward neural network, also called \textit{multilayer perceptron}, approximates some function \(f'\) by learning the values of parameters \(\mathbf{\theta}\) such that \(f(\mathbf{x} ; \mathbf{\theta}) \approx f'\).
Each layer \(\bm{h^{(l)}}\)  in the feedforward neural network consists of a linear combination of the previous layer \(\bm{h^{(l-1)}}\), followed by a non-linear transformation \(g^{(l)}\) such that \[\bm{h^{(l)}} = g^{(l)}(W^{(l)}\bm{h^{(l-1)}} + \bm{b^{(l)})},\]
where \(W^{(l)}\) represents the weight matrix and \(\bf{b^{(l)}}\) the bias allowing a shift in the activation. The logistic sigmoid is a very commonly used activation function: \[g^{(l)}(a) = \sigma(a) = \frac{1}{1+e^{-a}}\]
Other common functions include the Rectified Linear Unit (ReLu), \(g^{(l)}(a)= \max(0, a)\), the hyperbolic tangent, \(g^{(l)}(a)={\frac {e^{a}-e^{-a}}{e^{a}+e^{-a}}}\), or the radial basis function, \(g^{(l)}(a)=\varphi (\left\|a -c \right\|)\).
\subsection{Training}
For a given dataset \(X\), the typical training procedure of a feedforward neural network parameterized by \(\theta\) is gradient-based and consists of a series of steps: \\
\begin{enumerate}
    \item{ \bf{Defining a differentiable loss function}} \vspace{0.2cm} \\
    The choice of the loss function \(E_{\theta}(\mathbf{X})\) is an important aspect of network training and is typically selected such that \(E_{\theta}(\mathbf{X})\) is small when \(f_{\theta}(\mathbf{X}) \approx f'\). In a probabilistic interpretation, the appropriate loss function for a given problem can be derived from the principle of maximum likelihood estimation. For regression problems, maximising the likelihood by assuming a Gaussian noise distribution \(\epsilon\sim N(0,1)\) with 0 mean and unit variance on the output yields the so-called \textit{mean-squared (MSE) loss}. For \(N\) samples with input feature \(\mathbf{x}_{i}\) true label \(\mathbf{y}_{i}\) we define it as
    
\begin{equation}
MSE=\frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{y}_{i}-f_{\theta}(\mathbf{x}_{i}) \right\|^{2}
\end{equation}

In the context of binary classification, we interpret the squashed output scalar \({y}_{i}\) of the network as a probability and assume that the data are independent and identically distributed, yielding the likelihood
\begin{equation}
L(X)=\prod_{i=1}^{N}\left[f_{\theta}\left(\mathbf{x}_{i}\right)\right]^{\left({y}_{i}\right)}\left[1-f_{\theta}\left(\mathbf{x}_{i}\right)\right]^{\left(1-{y}_{i}\right)}
\end{equation}
    
For mathematical convenience, we take the logarithm of the expression and obtain our classification loss function, the \textit{cross-entropy loss}.

\begin{equation}
CE = -\sum_{i=1}^{N} y_{i} \log \left(f_{\theta}\left(\mathbf{x}_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-f_{\theta}\left(\mathbf{x}_{i}\right)\right)
\end{equation}

We can generalise to multi-class classification, which we are using in section \ref{methodology}, by one-hot encoding our labels \(\mathbf{y}_{i}\) for \(K\) classes:

\begin{equation}
   CE = -\sum_{i=1}^{N} \sum_{j=1}^{K} y_{i j} \log \left(f_{\theta}\left(x_{i}\right)_{j}\right)+\left(1-y_{i j}\right) \log \left(1-f_{\theta}\left(x_{i}\right)_{j}\right)
\end{equation}

    
    \item{ \bf{Computing the loss gradients} } \\
    \textit{Backpropagation} (also known as \textit{back-propagation of error}, or \textit{back-prop}) \cite{rumelhart_learning_1986} is by far the most popular and widely used algorithm for computing the gradients of the loss function of neural networks. Backpropagation is an instance of automatic reverse-mode differentiation and gained its popularity from the fact that it can accumulate all of the partial derivatives of any function with respect to all of its input in a very efficient manner. 
    
    \todo[inline]{Briefly explain backpropagation}
     \(\diffp{E(X,\theta)}{\theta}\)
    \item{ \bf{Optimization} } \\ 
    A final optimization procedure to perform learning using the gradient, i.e minimize the error function.
    \todo[inline]{Briefly explain ADAM and SGD}
\end{enumerate}

  



\subsection{Regularization}
\todo[inline]{Dropout etc ...}



\section{Related Work}
The three most prevalent applications of deep learning to drug discovery are \textit{de novo} drug design, drug-target interaction prediction (DTI), and compound property and activity prediction. \\ 
 \textit{De novo} drug design takes a generative approach by using neural networks to design novel molecules with certain desired properties. Gomez-Bombarelli \textit{et al.} proposed a method \cite{gomez-bombarelli_automatic_2018} that uses variational autoencoders (VAE) to learn continuous latent representations of string-encoded chemical structures coined SMILES (Simplified molecular-input line-entry system). After identifying optimal solutions in the latent space, the continuous vector representations are transformed back into string-encoded chemical structures, hereby generating new potential compounds.
 However, this method has the disadvantage of potentially yielding invalid molecules. Kusner \textit{et. al.} addressed this issue by introducing \textit{grammar variational autoencoders} (GVAE) \cite{kusner_grammar_2017} which
encode and decode directly to and from parse trees from a \textit{context-free grammar} (CFG). Jaques \textit{et al.} built on the success of recurrent neural networks (RNNs) in the field of natural language processing and \textit{Reinforcement Learning} (RL). They generated SMILES strings using RNNs and applied the RL method \textit{Deep Q-Learning} to incite desired, domain-specific properties \cite{jaques_sequence_2017}. \\
Drug-target interaction prediction (DTI) is another prominent approach in drug discovery and is concerned with the recognition of interactions between chemical compounds and the protein targets in the human body. Modulation of protein structure and function induced by interactions with compounds can have dramatic effects on the body as proteins are involved in a plethora of biological processes and pathways. Therefore, accurate interaction predictions can yield important discoveries on suitable drug candidates.
Feng \textit{et al.} employed deep learning methods to predict DTI. They combined Molecular Graph Convolution (MGC) for compound featurization with protein descriptors to predict real-valued interaction strength between compounds and proteins \cite{feng_padme_2019}.
Ragoza \textit{et al.}  proposed a method for protein-ligand scoring that applies a \textit{Convolutional Neural Network} (CNN) to three-dimensional (3D) representations of protein-ligand interactions as input. The scoring function automatically learns the key features of the protein-ligand interactions that correlate with binding \cite{ragoza_proteinligand_2017}.

 \todo[inline]{Add third application: Actitvity and Property prediction and related to my work}
  \todo[inline]{Related Work on Explainability}


\chapter{Experimental Setup}\label{methodology}

\section{Data}
Our data is provided by the \textit{Connectivity Map}, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS) and was generated by treating a sample of human cells with different drugs and then analyzing the cellular responses. \\ Cellular responses were measured in terms of expression levels of 772 landmark genes using the high-throughput gene expression assay L10000 and the viability values of 100 individual cell lines using the PRISM assay.\\
Formally, the data can be denoted as \(\mathscr{D}=\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}_{i=1}^{27796}\) where each tuple represents a drug perturbation, i.e a single treatment of a sample of cells by a specific drug.
\\ Each feature vector \(\mathbf{x}^{(i)}=[\mathbf{e}^{(i)},\mathbf{v}^{(i)}, c^{(i)},d^{(i)},t^{(i)}]^{T}\) consists of 
\begin{itemize}
    \item the expression levels \(\bf{e} \in \mathbb{R}^{772}\) of the 772 landmark genes
    \item the cell viability values \(\bf{v} \in \mathbb{R}^{100}\) of the 100 different cell lineages
    \item the variable \(c \in \{0,1\}\) indicating control perturbations (\(c=1\)) or drug treatments (\(c=0\))
    \item the dose \(d \in \{0,1,2\}\)
    \item the time (in hours) \(t \in \{24,48,72\}\) of measurement after adding the compound
    
\end{itemize}

\(\mathbf{y}^{(i)}=[y_{1}^{(i)},y_{2}^{(i)},\ldots, y_{k}^{(i)}]^{T}\) denotes a one-hot encoded vector of the mechanisms of action \(y_{i}\) associated with the drug. A single perturbation can have multiple mechanisms of actions assigned


The gene expression and cell viability data was first pre-processed using quantile normalization in order to standardize the feature values across plates. Then, a robust z-score was computed for each gene based on control perturbation (drugs without any mechanism of action) values, i.e. normalized for difference from the controls (\textit{differential expression}).
\todo[inline]{cut-off at +-10}

\section{Software}

\section{Problem and Task Definitions}
\begin{enumerate}
    \item Find patterns in data of Exploratory Data Analysis
    \item Train and optimize a model to infer mechanism of action from peturbational data. More formally, we want to learn a functional mapping \[f: \mathbb{R}^{L} \rightarrow \mathbb{R}^{K} \text { that fits }\left\{\mathbf{x}_{i}, \mathbf{y}_{i}\right\}_{i=1}^{N}\].
    \item Analyse and interpret the model as well as the predictions in the biological domain 
\end{enumerate}



\chapter{Exploratory Data Analysis}
\section{Distributions}\label{data distribution}

% \begin{figure}[h!]
% \caption{Distributions of controls, dose and time.}
% \includegraphics[height=6cm]{images/dist_plot.png}\label{cat_dist}
% \end{figure}
\begin{figure}[h!]
\caption{Distributions of label counts}
\includegraphics[height=6cm]{images/label_counts.png}\label{label_counts}
\end{figure}
\begin{figure}[h!]
\centering
\caption{Distribution of labels}
\includegraphics[height=15cm]{images/label_dist.png}\label{label_dist}
\end{figure}

\section{Correlation Analysis}
\subsection{Genes}
\begin{figure}[h!]
\centering
\caption{Gene correlations}
\includegraphics[height=9cm]{images/gene_corr.png}\label{gene corr}
\end{figure}
\subsection{Cells}
\begin{figure}[h!]
\centering
\caption{Cell correlations}
\includegraphics[height=9cm]{images/cell_corr.png}\label{gene corr}
\end{figure}
\subsection{Gene-Cells}
In order to investigate the relationship between gene expression and cell viability as a consequence of drug perturbation we computed for each sample the mean of the gene expression vector \(\mathbf{e}\) and cell viability vector \(\mathbf{v}\) respectively. We then examine pairs of scatter plots with signatures from cytotoxic perturbations (i.e. low cell viability mean) and non-cytotoxic perturbations (i.e high cell viability mean) respectively. We made consistent observations as exemplified in Figure \ref{low_mean} and \ref{high_mean}: An increase in cytotoxicity of a compound  seems to be associated with an increase in the variability of expression levels across genes.

In order to statistically solidify this observation, we  measured the Pearson correlation between the cell viability mean and the standard deviation of the gene expression means and could identify a statistically relevant correlation of -0.8836 (\(p\leq1e-300\)). This suggest that cell death is strongly associated with changes in gene expression.
We further found that the cell viability does not correlate with the gene expression mean (\(p\leq1e-300\)) but, instead, correlates highly with the absolute gene expression mean (\(p\leq1e-300\)) indicating that perturbation induced cell death is not associated with up- or down- regulation of genes specifically but with regulation in general. 

\begin{figure}[h!]
\centering
\caption{Heatmap of Pearson Correlations}
\includegraphics[height=7cm]{images/gene_cell_corr.png}\label{corr_map}
\end{figure}
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  cell\_mean &  cell\_std &  gene\_mean &  gene\_std \\
\midrule
cell\_mean &   1.000000 & -0.729786 &   0.079834 & -0.883603 \\
cell\_std  &  -0.729786 &  1.000000 &  -0.052747 &  0.755585 \\
gene\_mean &   0.079834 & -0.052747 &   1.000000 &  0.010411 \\
gene\_std  &  -0.883603 &  0.755585 &   0.010411 &  1.000000 \\
\bottomrule
\end{tabular}
\caption{Correlations between gene and cell statistics}\label{cell_gene_corr_table}
\end{table}

\begin{figure}[h!]
\centering
\caption{Scatter plot of sample with low cell viability mean}
\includegraphics[height=5cm]{images/low_cell_mean.png}\label{low_mean}
\end{figure}
\begin{figure}[h!]
\centering
\caption{\textbf{Scatter plot of sample with high cell viability mean.} The x-axis shows the individual features of the sample and the y-axis the associated feature values.}
\includegraphics[height=5cm]{images/high_cell_mean.png}\label{high_mean}
\end{figure}
\section{Discussion}
The directionality of the causal relationship between regulation and cell death cannot be established from the correlations alone, but studies suggest that transcription factors may be involved in regulating cell death and profileration and that, hence, the expression signatures may be predictive of the cell viability signatures. Szalai \textit{et al.} studied similar petrurbation-based gene expression and cell viability signatures and identified pathway and transcription factor activities associated with the gene–cell viability correlations using different statistical tools. Some of the most activated (e.g. TP53, FOXO3) and inactivated (e.g. E2F1, FOXM1 and MYC) transcription factors were found to exhibit causal roles as regulators in cell death and proliferation \cite{szalai_signatures_2019}.

These findings bear significance for the inference of mechanism of action from perturbation signatures because the expression of several genes acts as a confounding factor.


\section{Dimensionality Reduction}
\begin{figure}[h!]
\centering
\caption{PCA}
\includegraphics[height=9cm]{images/pca_labelled.png}\label{pca map}
\end{figure}
Feature and label correlations
Heatmaps, feature and label distributions
PCA
\clearpage
\chapter{Model Training}\label{model_training}
In this section, we are concerned with learning the functional mapping \[f: \mathbb{R}^{L} \rightarrow \mathbb{R}^{K} \text { that fits }\left\{\mathbf{x}_{i}, \mathbf{y}_{i}\right\}_{i=1}^{N},\] hereby constructing a model that predicts a set of mechanism of actions encoded as a binary vector for a given drug perturbation. 
This task can essentially be described as a \textit{multi-label classification} problem which is a generalization of \textit{multinomial classification} and requires more thought and adaptions in the sampling, learning and evaluation methods than the latter counterpart.\\
\section{Data Partitioning}
In order to obtain an generalizable estimate of the performance of our model, we can't compute evaluation metrics on the same dataset as was used for training or optimization as this would result in a biased score that is unlikely to generalize to future, unseen data. In the context of drug action prediction, such a biased score would not give us an accurate assessment of the robustness of our model and the ability to predict the MoA for new unknown compounds. 
We therefore partition our dataset \(\mathscr{D}\) into three disjoint subsets: A training set (70\% of the samples) for fitting the parameters of the model, a validation set (15\% of the samples) for optimizing the hyperparameters which are parameters controlling the learning process and finally a test set (15\% of the samples) on which we evaluate the optimized models in order to provide an approximation of the generalization error.
There exists other data partitioning methods such as \textit{cross-validation} in which a dataset is repeatedly split into a training dataset and a validation dataset that have shown to yield higher average predictive performance that simple splitting \cite{schaffer_selecting_1993}. However, it is still debated whether this method also results in less bias. \\
In our study, the fairly high number of samples reduced the need for cross-validation and our computational restrictions made the expensive cross-validation method rather infeasible. \\
Another requirement for the estimate of generalisation error is that the subsets follow a similar distribution. As show in \ref{data distribution}, the distribution of labels in our dataset is very imbalanced. This is commonly mitigated using stratified sampling in the partition of the datasets. While the implementation of this method for the multinomial classification problem is clear (e.g. groups are differentiated based on the value of the target variable), it is not evident how this would work for our multi-label case. We implement the novel \textit{iterative stratification} algorithm proposed by Sechidis \textit{et al.}. In each iteration it greedily examines the label with the fewest remaining examples in order to avoid that samples may be distributed in an undesired way if rare labels are not examined in priority. For each example of this label, it then selects an appropriate subset for distribution according to empirically validated heuristics \cite{sechidis_stratication_nodate}.
\section{Evaluation Measures}
Evaluation measures are critical to the assessment of the performance of a model and there exists trade-offs between different evaluation measures. Its choice can lead to widely diverging interpretations and is therefore of great importance.

The two desirable outcomes of the classification problem are 
\begin{itemize}
    \item True Positive: Correctly predicting that a drug has a certain effect.
    \item True Negative: Correctly predicting that a drug does not have a certain effect.
\end{itemize}

The two two types of errors to avoid are:
\begin{itemize}
    \item False Positive: Wrongly assigning a specific effect to a drug.
    \item False Negative: Failing to predict an effect of a drug.
\end{itemize}

Most metrics used for evaluation of classifiers are a function of these outcomes. In the context of multi-label classification, we additionally need to average the metrics across labels.

Accuracy is the most common evaluation criterion and informally describes the ratio of the number of correct predictions over the total number of predictions:
\[\text { Accuracy }=\frac{T P+T N}{T P+T N+F P+F N}\]

where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.

Accuracy, however, is not well suited to class-imbalanced problems like ours. A classifier can reach high accuracy scores on imbalanced data by merely predicting the majority class. Accuracy does not distinguish between the numbers of correctly classified examples of different classes \cite{galar_review_2012}.

A better indication of model performance is given by the Precision-Recall Curve which demonstrates the tradeoff between precision and recall for different thresholds. A high area under the curve (AUC) is desirable as it signifies both high recall and high precision.

\[\text { Precision }=\frac{TP}{TP+ FP}\]
\[\text { Recall }=\frac{TP}{TP+ FN}\]



\section{Baselines}
Baselines are critical when approaching a machine learning problem as they guide our further experiments and provide a point of reference from which to compare other algorithms.\\
We implement two different baselines with increasing complexity. The first baseline is a trivial solution and just predicts the most frequent label.
For the second baseline, we chose a simple linear classifier and implemented multinomial regression with a single-layer perceptron with a sigmoidal output activation function.
We trained both baselines on the training set and evaluated the models on the testing set. As seen in Table \ref{baseline_table}, the simple linear classifier strongly outperforms our trivial baseline.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{Model} &  Train Loss &  Test Loss & Train AUC & Test AUC  \\
\midrule
Majority Label Classifier &  0.0801 & 0.0574 & 0.0052 & 0.0037  \\
Multinomial Regressor  & 0.0128 & 0.0206 & 0.4290 & 0.1075 \\
2-Hidden Layer MLP & 0.0080 & 0.017 & 0.6885 & 0.1533 \\
\bottomrule
\end{tabular}
\caption{Baseline Results}\label{baseline_table}
\end{table}

Given that the ground-truth values of the model are mechanisms of actions which represent rather abstract concepts, we conjecture that additional hidden layers could be helpful for the learning algorithm to learn more abstract and hereby more discriminative representations. 
We therefore construct an extension of the sigmoidal single-layer perceptron by adding two hidden layers with Rectified Linear Units (ReLus). The first hidden layer maps the input features onto a 2048-dimensional latent space and the second hidden layer maps the latter onto a 1048-dimensional latent space. The new MLP achieved an AUC of 15.33\% on the test set.
\section{Discussion}

\chapter{Bayesian Hyperparameter Optimization}
Bayesian Optimization (BO) is a popular approach to optimizing objective functions with costly evaluation. It builds a surrogate model for the objective function and quantifies the uncertainty in that surrogate using a stochastic process, in practice usually a Gaussian Process (GP) \cite{frazier_tutorial_2018}.
In the context of our task, the objective function \(f\) is the AUC on the validation set after training our neural network on a set of hyperparameters \(\mathbf{x}\). 
Let \(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n} \in \mathbb{R}^{d}\) be the observed points after \(n\) trials and \(\mathbf{y}_{n}=\left(y_{1}, \ldots, y_{n}\right)^{\top}\) be the corresponding observations of the function \(f\).
By the Gaussian process model, the observation \(y\) for a new set of hyperparameters \(\mathbf{x}\) is normally distributed, \(y \sim \mathcal{N}\left(\mu_n(\mathbf{x}), \sigma_n^{2}(\mathbf{x})\right)\). The predictive mean and  variance functions are defined as:
\[\mu_{n}(\mathbf{x})=\mathbf{k}_{n}(\mathbf{x})^{\top}\left(\mathbf{K}_{n}+\sigma_{n}^{2} \mathbf{I}\right)^{-1} \mathbf{y}_{n}\]
\[\sigma_{n}^{2}(\mathbf{x})=k(\mathbf{x}, \mathbf{x})-\mathbf{k}_{n}(\mathbf{x})^{\top}\left(\mathbf{K}_{n}+\sigma_{n}^{2} \mathbf{I}\right)^{-1} \mathbf{k}_{n}(\mathbf{x}),\]
where
\(\mathbf{k}_{n}(\mathbf{x})=\left(k\left(\mathbf{x}_{1}, \mathbf{x}\right), \ldots, k\left(\mathbf{x}_{n}, \mathbf{x}\right)\right)^{\top}\)
and \(\left(\mathbf{K}_{n}\right)_{i j}=k\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)\). \(k\) is our chosen kernel function, representing the covariance of a pair of observations. \(k\) is commonly chosen to be an exponentiated quadratic kernel with overall variance \(\sigma^{2}\) and lengthscale \(\ell\) :
\[k\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\sigma^{2} \exp \left(-\frac{\left\|\mathbf{x}_{i}-\mathbf{x}_{j}\right\|^{2}}{2 \ell^{2}}\right)\]

However, for the purpose of practical hyperparameter optimization \textit{Snoek et al.} \cite{snoek_practical_nodate} observe that the kernel is "unrealistically smooth" and instead propose the Matern 5/2 kernel 
\[k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\theta_{0}\left(1+\sqrt{5 r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)}+\frac{5}{3} r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right) \exp \left(-\sqrt{5 r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)}\right)\]

The Gaussian prior and the observations induce a posterior over functions which help us decide where to next evaluate \(f\) using the inexpensive acquisition function \(a\). The next point \(\mathbf{x}_{n+1}\) is determined via the proxy optimization  \(\mathbf{x}_{n+1}=\operatorname{argmax}_{\mathbf{x}} a(\mathbf{x})\).
A common choice of acquisition function is the \textit{Expected Improvement}
\[
a_{\mathrm{EI}}(x)=\mathbb{E}[f(\mathbf{x})-f_{n}^{*} \mid\left(\mathbf{x}_{1}, y_{1}, \ldots, \mathbf{x}_{n}, y_{n}\right)],
\] where \(f_{n}^{*}\) is the maximal value of \(f\) observed so far.
\section{Experimental Setup}
One of the most crucial aspects of experimental setup of Bayesian Optimisation is the search space. It involves the choice of the hyperparameters to optimize and their domain and is subject to the trade-off between computational resources and time. Additionally, BO currently has scalability issues and is practically limited to a search space involving 10 - 20 parameters \cite{moriconi_high-dimensional_2020}.

We take a heuristic approach to the optimization of the neural network architecture by defining three parameters that describe the entire architecture rather than exploding the dimensionality of the search space with parameters describing the activation functions and number of nodes for each layer of the network:
\begin{itemize}
    \item The activation function for all hidden layers.
    \item The number of hidden layers in the network.
    \item The \textit{node decreasing factor} describing the decrease in the number of nodes from layer to layer. Let \(|l_i|\) be the number of nodes in layer \(i\), where \(l_0\) is the input layer, and \(d\) be the node decreasing factor, then the number of nodes layer \(i\) are described by \( |l_i| = d|l_{i-1}| \).
\end{itemize}
This relaxed neural architecture optimization follows from the assumption that the number of nodes need to decrease in order to learn biologically meaningful and discriminative generalisations of the correlated biological data. \\
Aside from architecture parameters, we optimize the 
\begin{itemize}
    \item batch size
    \item epochs
    \item learning rate of the learning procedure
    \item the dropout rate
    \item boolean parameter indicating whether \textit{batch normalization} should be applied to the layers. Batch normalization standardizes the activations of each input variable per mini-batch with the aim of stabilizing and speeding-up the learning training procedure.\\
\end{itemize}

Table \ref{search_sapce} shows the search space including the domain of each parameter.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{Hyperparameter} &  Domain  \\
\midrule
Hidden layers & {1,2,3} \\
Node decreasing factor & [0.2,0.95] \\
Activation function & {ReLU, SeLU, Swish} \\
Batch size & {32,64,128,256} \\
Epochs & {15,35,45} \\
Dropout & [0.1,0.8] \\
Learning rate & [1e-6,1e-1]  \\
Batch normalization & {True, False} \\
\bottomrule
\end{tabular}
\caption{Domain for hyperparameter optimization}\label{search_sapce}
\end{table}

\section{Results}
We performed 74 training runs with parameters sampled according to the acquisition function and evaluated the runs on the validation set. The best parameters found are shown in Table \ref{bo_results}.
The set of parameters achieved a validation AUC of 0.197 and a test AUC of 0.17.
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{Hyperparameter} &  Best value  \\
\midrule
Hidden layers & 2 \\
Node decreasing factor & 0.78 \\
Activation function & ReLU \\
Batch size & 32 \\
Epochs & 35 \\
Dropout & 0.22 \\
Learning rate & 0.0017  \\
Batch normalization & False \\
\bottomrule
\end{tabular}
\caption{Best parameters identified by BO}\label{bo_results}
\end{table}
% \begin{figure}[h!]
% \centering
% \caption{Evaluations of BO}
% \includegraphics[height=16cm]{images/bo_evaluations.png}\label{bo_eval}
% \end{figure}
Figure \ref{bo_obj} shows the partial dependence plot of the objective function and is calculated by averaging the value of the objective function a set of random samples in the search-space, while keeping one or two dimensions fixed at regular intervals. 
\begin{figure}[h!]
\centering
\caption{\textbf{Partial dependence plots of objective function.} Yellow regions are better and blue regions are worse, the red star indicates the optimum of the objective function.}
\includegraphics[height=16cm]{images/bo_objective.png}\label{bo_obj}
\end{figure}
\section{Discussion}
The dependence plots indicate that a smaller number of hidden layers with a high node decreasing factor are favoured, while the node decreasing factor becomes increasingly important with the number of hidden layers.


We perform relaxed neural architecture search using a constant node decreasing factor, hereby assuming the optimal number of nodes to be exponentially decaying in the number of layers. In further experiments, we could extend the node decreasing factor to be a different function of the number of layers.
It should be noted the surrogate model and dependence plots can be inaccurate because they are built from only 74 samples of calls to the fitness function. The results should be validated on a larger set of trials.


\chapter{Model Interpretation}
One of the biggest obstacles to the adoption of deep learning in biomedical applications is its lack of interpretability. Without the interpretabiliy criterion met, physicians and other medical professionals cannot justify taking a decision based solely on the output of a black box algorithm as patients' lives may be at stake. In the context of our work, understanding why a compound is attributed a certain mechanism of action and which genes, pathways and biological processes led to the classification is critical to being able to move forward with the costly and sensitive drug development process. Interpretability further ensures that a neural network bases its predictions on reliable representations and signals of the data. Finally, we are interested in the patterns identified by the neural network and whether they are able to generate new biological insights.

\section{Integrated Gradients}
In this section, we want to investigate why a specific compound was assigned a particular mechanism of action and how it achieves that particular MoA.
For instance, the MoA of anti-inflammatory drugs such as Aspirin and Ibuprofen is \textit{cyclooxygenase inhibitor} which describes the inhibition of the inflammation inducing cyclooxygenase enzymes. However, the way Ibuprofen and Aspirin achieve the inhibition of these enzymes can involve completely different pathways and biological processes. Identifying these can help us assess the efficacy, safety and side effects of the drug.

Recent works in methods for interpreting deep neural networks show that gradient methods produce better interpretations of a neural network than simple weight analysis \cite{montavon_methods_2018}. Backpropagating the activation of the output neuron and analysing the product of the gradient and feature values is a reasonable start for feature attribution \cite{baehrens_how_nodate}.\\
However, gradients can be problematic when activations have a near-zero gradient at high or low inputs such as with the ReLU activation function \cite{shrikumar_not_2017}. The prediction function may flatten at the input and thus have zero gradient despite the significance of the input.
\textit{Sundararajan et al.} \cite{sundararajan_axiomatic_2017} formalize this requirement to feature attribution methods as the \textit{Sensitivity Axiom}. They establish that "an attribution method satisfies Sensitivity if for every input and baseline that differ in one feature but have different predictions, then the differing feature should be given a nonzero attribution" \cite{sundararajan_axiomatic_2017}.
Several methods were proposed to tackle this issue. The \textit{DeepLift} method, for instance, satisfies the sensitivity axiom by computing feature importance based on the difference in activation from a baseline or reference input which may still be nonzero even when the gradient is zero \cite{sundararajan_axiomatic_2017}.\\
Unfortunately, these improved methods violate another requirement of attribution methods, namely \textit{Implementation Invariance}. The axiom of Implementation Invariance states that attributions for two functionally equivalent networks, i.e., structurally different networks producing the same outputs for every input, should be equivalent. Its violation implies that attributions of these methods are potentially sensitive to unimportant aspects of the model. \cite{sundararajan_axiomatic_2017}. \\
Integrated Gradients, a method proposed by \textit{Sundararajan et al.}, addresses these issues by defining an attribution value for each feature by considering the integral of the gradients taken along a straight path from a baseline instance \(x^{\prime}\) to the input instance \(x\) \cite{sundararajan_axiomatic_2017}. \\

We define this formally for our problem of identifying the integrated gradients of the gene and cell values for a given MoA classification.

For our learned function, \[f: \mathbb{R}^{L} \rightarrow \mathbb{R}^{K},\] we define \[f_{k}: \mathbb{R}^{L} \rightarrow \mathbb{R}\] as a mapping to the probability of the indexed label \(k\), which could be one of the ground-truth MoA labels. The attributions \(A_{i}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\) towards the label \(k\) for each feature \(x_{i}\) with respect to the corresponding feature \(x_{i}^{\prime}\) in the baseline are calculated as
\[ A_{i}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left(x_{i}-x_{i}^{\prime}\right) \int_{0}^{1} \frac{\partial f_{k}\left(\mathbf{x}^{\prime}+\alpha\left(\mathbf{x}-\mathbf{x}^{\prime}\right)\right)}{\partial x_{i}} d \alpha \]

\section{Results}
For our experimental setup, we select samples for different labels uniformly at random and compute the attributions for each gene and cell feature with respect to the mean control perturbation as baseline.

In order to select the features with highest attributions towards the MoA label for further analysis, we cannot just take the absolute values of the attributions as this would lead to misleading results. For instance, a negative attribution could both signify the down-regulation of a gene contributing to the label attribution and the up-regulation of a gene negatively contributing to the label attribution. It is imperative to keep the former and remove the latter from our selection.

We filter our array of attributions by only keeping the attributions where the sign of the integrated gradient matches the sign of the feature activation.

We then select a subset of the 50 highest absolute attributions and extract the gene identifiers to perform \textit{Reactome} pathway enrichment analysis \cite{fabregat_reactome_2017}. Reactome is a peer-reviewed pathway database and the enrichment provides us with the statistically over-represented pathways in which the genes in the subset are involved, hereby enabling a biological interpretation of the integrated gradients.
We then compare the statistically significant (\(p < 0.05\)) pathways with literature associating these pathways with the drugs used in the respective samples.

In order to quantify our results, we define an enrichment of the attributions of a drug sample as being confirmed by the literature if in the 5 pathways with highest statistical significance,  at least 2 pathways are found to be associated with the drug and its MoA in the literature.

We conduct the above described method on a set of 20 samples with distinct MoA. The MoAs with highest label count were selected in order to mitigate overfitting caused by MoA with very few samples in the training set. We report a confirmation of enrichment for 9 out of 20 samples. 

\section{Discussion}
We demonstrate our method on an example perturbation sample by \textit{Bortezomib} and its associated target \textit{Proteasome Inhibitor.} Proteasomes are protein complexes which degrade unneeded or damaged proteins by proteolysis. The proteasome pathways are critical for the proliferation and survival of all cells and appear to be associated with the pathophysiology of some cancers. It was therefore suggested that pharmacological inhibition of proteasome function may prove useful as a novel class of anticancer drugs \cite{chen_bortezomib_2011}. The proteasome inhibitor Bortezomib has shown to induce apoptosis of a variety of cancer cells, including leukemia, lymphoma, multiple myeloma and breast cancers \cite{cao_ubiquitin-proteasomal_2011}.
Table \ref{bortezomib} shows the 5 most significant pathways for the gene attributions. We compared the identified pathways with the literature on the pharmacodynamics of Bortezomib and could find at least two pathway associations in the literature. 
The over-represented Telomere pathways in Table \ref{bortezomib} align with findings by \textit{Weiss et al.} that Bortezomib downregulates telomerase activity in multiple myeloma cells \cite{weiss_differential_2012}.
We further find confirmation in the literature for the importance of \textit{Histone deacetylases} (HDACs) as found by our over-representation analysis. Histone deacetylases are enzymes involved in chromatin compaction and transcriptional repression \cite{yang_rpd3hda1_2008}. \textit{Kikuchi et al.} conclude that Histone deacetylases are critical targets of bortezomib-induced cytotoxicity in multiple myeloma \cite{kikuchi_histone_2010}.\\
\begin{table}[h!]
\centering
\begin{tabular}{p{8cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
Pathway name &	\#Entities found &	\#Entities total &	Entities ratio &	Entities pValue\\
\midrule
Telomere Maintenance&	3&	111&	0.0075	&0.0009 \\
Chromosome Maintenance&	3&	138 &	0.0094&	0.0018 \\
TGFBR2 MSI Frameshift Mutants in Cancer &	1 &	2&	0.0001&	0.0035 \\
E3 ubiquitin ligases ubiquitinate target proteins&	2&	61&	0.0041&	0.0052\\
HDACs deacetylate histones&	2&	67&	0.0045&	0.0062\\
Extension of Telomeres&	2&	69&	0.0046&	0.0066 \\
\bottomrule
\end{tabular}
\caption{Enriched pathways for Bortezomib}\label{bortezomib}
\end{table}

To conclude, we should note that the results reported in the previous section should be validated by further experiments. We conducted our method on a set of only 20 samples as our work is severely time-limited and the method requires significant manual evaluation for the literature research.
Furthermore, it should be noted that the 45\% confirmation ratio for our samples does not suggest that the model or the integrated gradients method is unreliable  as our model looks for correlations between the output and the input and not for causal relationships. Enriched pathways that are not confirmed by the literature could either be indirectly correlated with the MoA or exhibit an unknown causal relationship.  Many drugs are still objects of active research and it is very possible that relationships proposed by our model will be confirmed by research community in the future. \\
Additionally, we should also consider that biological databases such as Reactome are by no means perfect representations of biological knowledge. A lot of information will be missing and hence not appear in our enrichment analysis. \\
Lastly, both positive and negative results are implicated by overfitting and training bias as demonstrated in \ref{model_training}. Especially mechanisms of actions with a lower number of samples in the training set could produce pathway enrichments that are related to noise in the sample and not a significant biological function. \\
We therefore conclude that the proposed model interpretation method should not be used as a tool to evaluate the reliability and robustness of the model but rather as a tool for biological discovery that can generate biological hypotheses  and propose new drug effects that are then to be investigated and experimentally confirmed by biologists.




% We tackle this problem by determining the genes and cells that most contributed to the classification in the the neural network and then enrich these features with annotations from biological process and pathway databases. 


% By identifying the genes and cells that most contributed to the MoA classification, we can infer which biological processes achieve that MoA and whether we can i

% This question is of interest because it  I.e. we want to identify the biological processes


\chapter{Conclusions}

\chapter{Future Work}
Include domain specific data

Limitations
CMap comes with some limitations, such as limited drug perturbation data, a limited drug coverage, dosage-dependent conditions and the uncertainty of applying cell lines or animal model expression patterns to human systems. Also, the methodology can be expensive and time-consuming before it can generate a significant portion of all safe dosage conditions for a limited number of cell lines for CMap

% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}
