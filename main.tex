
% to choose your degree
% please un-comment just one of the following

\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}     % for BSc, BEng etc.
% \documentclass[minf,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}  % for MInf

\usepackage{bm}
\usepackage[thinc]{esdiff}
\usepackage{todonotes}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
%\reversemarginpar
\begin{document}

\title{An Interpretable Deep Learning Approach to Pharmacodynamic Mechanism of Action Prediction}

\author{Ragnor Comerford}

% to choose your course
% please un-comment just one of the following
\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Software Engineering}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Psychology }   
%\course{Artificial Intelligence with Psychology }   
%\course{Linguistics and Artificial Intelligence}    
%\course{Computer Science}
%\course{Software Engineering}
%\course{Computer Science and Electronics}    
%\course{Electronics and Software Engineering}    
%\course{Computer Science and Management Science}    
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}  
%\course{Computer Science and Statistics}    

% to choose your report type
% please un-comment just one of the following
\project{Undergraduate Dissertation} % CS&E, E&SE, AI&L
%\project{Undergraduate Thesis} % AI%Psy
%\project{4th Year Project Report}

\date{\today}

\abstract{
This is an example of {\tt infthesis} style.
The file {\tt skeleton.tex} generates this document and can be 
used to get a ``skeleton'' for your thesis.
The abstract should summarise your report and fit in the space on the 
first page.
%
You may, of course, use any other software to write your report,
as long as you follow the same style. That means: producing a title
page as given here, and including a table of contents and bibliography.
}

\maketitle

\section*{Acknowledgements}
I would like to express my sincere gratitude to my supervisor, Michael Herrmann, for his valuable guidance and insightful feedback. I would also like to thank my family and friends for their constant support and love
throughout my academic years.

\tableofcontents

%\pagenumbering{arabic}


\chapter{Introduction}

\section{Motivation}
Drug discovery has traditionally been an expensive and inefficient process with a very serendipitous approach to the identification of novel drug compounds.
Despite the huge scientific and technological advances such as high-throughput screening (HTS), parallel synthesis and combinatorial chemistry in the past decade, the biopharmaceutical industry has not seen much increase in efficiency. This development, coined \textit{Eroom's Law}, states that drug discovery has become slower and more expensive with a steadily decrease in efficiency since the 1980s \cite{scannell_diagnosing_2012}. But it is as much an issue of science as it is a social and political one. The cost of each new molecular entity (NME) has climbed to approximately US\$1.8 billion \cite{paul_how_2010}. as "the number of new drugs approved per billion US dollars spent on R\&D has halved roughly every 9 years since 1950, falling around 80-fold in inflation-adjusted terms"\cite{scannell_diagnosing_2012}.
The astronomical costs of drug development are reflected in the inequality of access to health care and disproportionately affects developing countries where a big proportion of the population is deprived of life-saving treatment. \\
However, recent advances in machine learning (ML) coupled with a significant increase in computing power and data are inducing a renaissance in drug development.
Deep learning (DL) \cite{lecun_deep_2015} with its ability to model complex non-linear relationships has successfully taken advantage of the exponential growth in the availability of data and computer hardware. The algorithmic optimization of DL algorithms for graphical processing units (GPUs) has significantly reduced the time to train DL models by leveraging the power of GPUs to significantly speed up parallel processing and matrix operations.
The explosive growth of available biomedical and compound activity databases \cite{papadatos_activity_2015}\cite{kim_pubchem_2016}\cite{gaulton_chembl_2012}, partly owed to the emergence of new experimental techniques such as HTS or parallel synthesis \cite{grada_next-generation_2013}, allows DL to perform automatic feature detection and complex representation learning from massive amounts of unlabelled or labelled biomedical data.\\
One such instance of large-scale biomedical data collection enabled by new experimental techniques is the \textit{Connectivity Map} (CMap) \cite{lamb_connectivity_2006} which plays a central role in our work. The CMap is a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the  NIH  Common  Funds  Library  of  Integrated  Network-Based  Cellular  Signatures(LINCS) that uses gene expression signatures to discover functional connections among diseases, genetic perturbation, and drug action \cite{musa_review_2017}. They developed the L1000 assay to facilitate rapid, flexible and high-throughput gene expression profiling at lower costs \cite{subramanian_next_2017} and built a comprehensive, large-scale drug perturbation database containing gene expression signatures of a wide range of cultivated cell lines treated with thousands of chemical compounds. In this work, we leverage the recent availability of such data as well as deep learning techniques to understand the relationship between drugs, genes and pathways and build an accurate model predicting the function of unknown compounds.


\section{Contributions}
\begin{itemize}
\item Literature review on existing approaches to drug action prediction 
\item Exploratory data analysis and PCA
\item Design and Implementation of a variational autoencoder for feature extraction
\item Design and implementation of neural networks for mechanism of action prediction
\item Implementation of Bayesian Optimization
\item Model and Prediction Interpretation
\item Causal Inference
\end{itemize}
\section{Outline of the report}

\chapter{Background}
In this chapter, we introduce the reader to the biological concepts and mechanisms that are relevant to the understanding of deep learning applications to drug discovery. This is followed by a brief introduction to deep learning algorithms. \\
We assume that the reader is familiar with the basic concepts of biology and the fundamentals of linear algebra, probability, and calculus. We refer the interested reader to Goodfellow \textit{et al.} (2016) \cite{goodfellow_deep_2016} and Alberts \textit{et al.} (2013) \cite{alberts_essential_2013} for comprehensive introductions.
\section{Molecular biology}
\subsection{Gene expression}
\textit{Gene expression} forms the fundamental part of the central dogma of microbiology \cite{crick_protein_1958} and refers to the process by which the genetic information stored in the DNA is used in the synthesis of a gene product such as RNA or protein. Gene expression can be regulated by a wide range of biological mechanisms in the cell. Activation or inactivation of signaling pathways in regular physiological processes or in response to stimuli affects gene expression via transcription factors and their regulatory genes \cite{itadani_can_2008}. This cascade of signal transduction generates characteristic patterns of gene expression, commonly referred to as \textit{gene expression signatures}.
Changes in gene expression as a consequence of disease or pharmacologic perturbation (termed \textit{differential gene expression signatures}) bears significant potential in improving our understanding of disease treatment and the identification of drug targets.
\subsection{Cell viability}
\subsection{Mechanism of Action}
The \textit{mechanism of action} (MoA) of a compound refers to the set of target and effector proteins that are associated with a certain biological activity in a specific cellular context. Its correct identification for novel compounds represents a major challenge in drug development as most candidate compounds fail in the pharmacological pipeline due to toxicity associated with off-target affects and lack of efficacy \cite{wehling_assessing_2009}.

\todo[inline]{Rewrite the following:}
In the case of anti-infective drug development, the information permits anticipation of problems relating to clinical safety. Drugs disrupting the cytoplasmic membrane or electron transport chain, for example, are more likely to cause toxicity problems than those targeting components of the cell wall (peptidoglycan or Beta-glucans) or 70S ribosome, structures which are absent in human cells.[4][5]
By knowing the interaction between a certain site of a drug and a receptor, other drugs can be formulated in a way that replicates this interaction, thus producing the same therapeutic effects. Indeed, this method is used to create new drugs.
It can help identify which patients are most likely to respond to treatment. Because the breast cancer medication trastuzumab is known to target protein HER2, for example, tumors can be screened for the presence of this molecule to determine whether or not the patient will benefit from trastuzumab therapy.[6][7]
It can enable better dosing because the drug's effects on the target pathway can be monitored in the patient. Statin dosage, for example, is usually determined by measuring the patient's blood cholesterol levels.[6]
It allows drugs to be combined in such a way that the likelihood of drug resistance emerging is reduced. By knowing what cellular structure an anti-infective or anticancer drug acts upon, it is possible to administer a cocktail that inhibits multiple targets simultaneously, thereby reducing the risk that a single mutation in microbial or tumor DNA will lead to drug resistance and treatment failure.[4][8][9][10]
It may allow other indications for the drug to be identified. Discovery that sildenafil inhibits phosphodiesterase-5 (PDE-5) proteins, for example, enabled this drug to be repurposed for pulmonary arterial hypertension treatment, since PDE-5 is expressed in pulmonary hypertensive lungs.[11][12]

\subsection{CMAP}
\section{Deep learning}
\textit{Deep learning} can generally be described as a class of machine learning algorithms that uses \textit{artificial neural networks} (ANNs) with multiple layers between the input and output layers for learning data representations. ANNs are loosely inspired by the biological brain where inter-connected neurons transmit signals to each other.
The universal approximation theorem represents an important discovery in the theory of deep learning. It states that, for any continuous function \(f\) on a compact space \(K\), there exists a single-hidden-layer feedforward neural network which uniformly approximates \(f\) within an arbitrary error \(\epsilon>0\) on \(K\) \cite{hornik_multilayer_1989}.

Why is deep learning helpful in drug discovery (features
Limitations
Recent advances
\subsection{Feedforward neural network}
A \textit{feedforward neural network} consists of an input layer and output layer that passes information in a forward fashion through a series of latent layers with the ultimate goal of approximating a function with a composition of intermediate computations. More formally, a feedfoward neural network, also called \textit{multilayer perceptron}, approximates some function \(f'\) by learning the values of parameters \(\boldsymbol{\theta}\) such that \(f(\boldsymbol{x} ; \boldsymbol{\theta}) \approx f'\).
Each layer \(\bm{h^{(l)}}\)  in the feedforward neural network consists of a linear combination of the previous layer \(\bm{h^{(l-1)}}\), followed by a non-linear transformation \(g^{(l)}\) such that \[\bm{h^{(l)}} = g^{(l)}(W^{(l)}\bm{h^{(l-1)}} + \bm{b^{(l)})},\]
where \(W^{(l)}\) represents the weight matrix and \(\bf{b^{(l)}}\) the bias allowing a shift in the activation. The logistic sigmoid is a very commonly used activation function: \[g^{(l)}(a) = \sigma(a) = \frac{1}{1+e^{-a}}\]
Other common functions include the Rectified Linear Unit (ReLu), \(g^{(l)}(a)= \max(0, a)\), the hyperbolic tangent, \(g^{(l)}(a)={\frac {e^{a}-e^{-a}}{e^{a}+e^{-a}}}\), or the radial basis function, \(g^{(l)}(a)=\varphi (\left\|a -c \right\|)\).
\subsection{Training}
For a given dataset \(X\), the typical training procedure of a feedforward neural network parameterized by \(\theta\) is gradient-based and consists of a series of steps: \\
\begin{enumerate}
    \item{ \bf{Defining a differentiable loss function}} \vspace{0.2cm} \\
    The choice of the loss function \(E_{\theta}(\mathbf{X})\) is an important aspect of network training and is typically selected such that \(E_{\theta}(\mathbf{X})\) is small when \(f_{\theta}(\mathbf{X}) \approx f'\). In a probabilistic interpretation, the appropriate loss function for a given problem can be derived from the principle of maximum likelihood estimation. For regression problems, maximising the likelihood by assuming a Gaussian noise distribution \(\epsilon\sim N(0,1)\) with 0 mean and unit variance on the output yields the so-called \textit{mean-squared (MSE) loss}. For \(N\) samples with input feature \(\mathbf{x}_{i}\) true label \(\mathbf{y}_{i}\) we define it as
    
\begin{equation}
MSE=\frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{y}_{i}-f_{\theta}(\mathbf{x}_{i}) \right\|^{2}
\end{equation}

In the context of binary classification, we interpret the squashed output scalar \({y}_{i}\) of the network as a probability and assume that the data are independent and identically distributed, yielding the likelihood
\begin{equation}
L(X)=\prod_{i=1}^{N}\left[f_{\theta}\left(\mathbf{x}_{i}\right)\right]^{\left({y}_{i}\right)}\left[1-f_{\theta}\left(\mathbf{x}_{i}\right)\right]^{\left(1-{y}_{i}\right)}
\end{equation}
    
For mathematical convenience, we take the logarithm of the expression and obtain our classification loss function, the \textit{cross-entropy loss}.

\begin{equation}
CE = -\sum_{i=1}^{N} y_{i} \log \left(f_{\theta}\left(\mathbf{x}_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-f_{\theta}\left(\mathbf{x}_{i}\right)\right)
\end{equation}

We can generalise to multi-class classification, which we are using in section \ref{methodology}, by one-hot encoding our labels \(\mathbf{y}_{i}\) for \(K\) classes:

\begin{equation}
   CE = -\sum_{i=1}^{N} \sum_{j=1}^{K} y_{i j} \log \left(f_{\theta}\left(x_{i}\right)_{j}\right)+\left(1-y_{i j}\right) \log \left(1-f_{\theta}\left(x_{i}\right)_{j}\right)
\end{equation}

    
    \item{ \bf{Computing the loss gradients} } \\
    \textit{Backpropagation} (also known as \textit{back-propagation of error}, or \textit{back-prop}) \cite{rumelhart_learning_1986} is by far the most popular and widely used algorithm for computing the gradients of the loss function of neural networks. Backpropagation is an instance of automatic reverse-mode differentiation and gained its popularity from the fact that it can accumulate all of the partial derivatives of any function with respect to all of its input in a very efficient manner. 
    
    \todo[inline]{Briefly explain backpropagation}
     \(\diffp{E(X,\theta)}{\theta}\)
    \item{ \bf{Optimization} } \\ 
    A final optimization procedure to perform learning using the gradient, i.e minimize the error function.
    \todo[inline]{Briefly explain ADAM and SGD}
\end{enumerate}

  



\subsection{Regularization}
\todo[inline]{Dropout etc ...}

\subsection{Variational Autoencoder}

\section{PCA}
\section{Bayesian Optimization}
\section{Related Work}
The three most prevalent applications of deep learning to drug discovery are \textit{de novo} drug design, drug-target interaction prediction (DTI), and compound property and activity prediction. \\ 
 \textit{De novo} drug design takes a generative approach by using neural networks to design novel molecules with certain desired properties. Gomez-Bombarelli \textit{et al.} proposed a method \cite{gomez-bombarelli_automatic_2018} that uses variational autoencoders (VAE) to learn continuous latent representations of string-encoded chemical structures coined SMILES (Simplified molecular-input line-entry system). After identifying optimal solutions in the latent space, the continuous vector representations are transformed back into string-encoded chemical structures, hereby generating new potential compounds.
 However, this method has the disadvantage of potentially yielding invalid molecules. Kusner \textit{et. al.} addressed this issue by introducing \textit{grammar variational autoencoders} (GVAE) \cite{kusner_grammar_2017} which
encode and decode directly to and from parse trees from a \textit{context-free grammar} (CFG). Jaques \textit{et al.} built on the success of recurrent neural networks (RNNs) in the field of natural language processing and \textit{Reinforcement Learning} (RL). They generated SMILES strings using RNNs and applied the RL method \textit{Deep Q-Learning} to incite desired, domain-specific properties \cite{jaques_sequence_2017}. \\
Drug-target interaction prediction (DTI) is another prominent approach in drug discovery and is concerned with the recognition of interactions between chemical compounds and the protein targets in the human body. Modulation of protein structure and function induced by interactions with compounds can have dramatic effects on the body as proteins are involved in a plethora of biological processes and pathways. Therefore, accurate interaction predictions can yield important discoveries on suitable drug candidates.
Feng \textit{et al.} employed deep learning methods to predict DTI. They combined Molecular Graph Convolution (MGC) for compound featurization with protein descriptors to predict real-valued interaction strength between compounds and proteins \cite{feng_padme_2019}.
Ragoza \textit{et al.}  proposed a method for protein-ligand scoring that applies a \textit{Convolutional Neural Network} (CNN) to three-dimensional (3D) representations of protein-ligand interactions as input. The scoring function automatically learns the key features of the protein-ligand interactions that correlate with binding \cite{ragoza_proteinligand_2017}.

 \todo[inline]{Add third application: Actitvity and Property prediction and related to my work}
  \todo[inline]{Related Work on Explainability}


\chapter{Methodology}\label{methodology}
\section{Introduction}
\subsection{Data}
Our data is provided by the \textit{Connectivity Map}, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS) and was generated by treating a sample of human cells with different drugs and then analyzing the cellular responses. \\ Cellular responses were measured in terms of expression levels of 772 landmark genes using the high-throughput gene expression assay L10000 and the viability values of 100 individual cell lines using the PRISM assay.\\
Formally, the data can be denoted as \(\mathscr{D}=\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}_{i=1}^{27796}\) where each tuple represents a drug perturbation, i.e a single treatment of a sample of cells by a specific drug.
\\ Each feature vector \(\mathbf{x}^{(i)}=[\mathbf{e}^{(i)},\mathbf{v}^{(i)}, c^{(i)},d^{(i)},t^{(i)}]^{T}\) consists of 
\begin{itemize}
    \item the expression levels \(\bf{e} \in \mathbb{R}^{772}\) of the 772 landmark genes
    \item the cell viability values \(\bf{v} \in \mathbb{R}^{100}\) of the 100 different cell lineages
    \item the variable \(c \in \{0,1\}\) indicating control perturbations (\(c=1\)) or drug treatments (\(c=0\))
    \item the dose \(d \in \{0,1,2\}\)
    \item the time (in hours) \(t \in \{24,48,72\}\) of measurement after adding the compound
    
\end{itemize}

\(\mathbf{y}^{(i)}=[y_{1}^{(i)},y_{2}^{(i)},\ldots, y_{k}^{(i)}]^{T}\) denotes a one-hot encoded vector of the mechanisms of action \(y_{i}\) associated with the drug. A single perturbation can have multiple mechanisms of actions assigned


The gene expression and cell viability data was first pre-processed using quantile normalization in order to standardize the feature values across plates. Then, a robust z-score was computed for each gene based on control perturbation (drugs without any mechanism of action) values, i.e. normalized for difference from the controls (\textit{differential expression}).
\todo[inline]{cut-off at +-10}

\subsection{Software}

\subsection{Problem and Task Definitions}
\begin{enumerate}
    \item Find patterns in data of Exploratory Data Analysis
    \item Train and optimize a model to infer mechanism of action from peturbational data. More formally, we want to learn a functional mapping \[f: \mathbb{R}^{L} \rightarrow \mathbb{R}^{K} \text { that fits }\left\{\mathbf{x}_{i}, \mathbf{y}_{i}\right\}_{i=1}^{N}\].
    \item Analyse and interpret the model as well as the predictions in the biological domain 
\end{enumerate}



\section{Exploratory Data Analysis}
\subsection{Distributions}\label{data distribution}

\begin{figure}[h!]
\caption{Distributions of controls, dose and time.}
\includegraphics[height=6cm]{images/dist_plot.png}\label{cat_dist}
\end{figure}
\begin{figure}[h!]
\caption{Distributions of label counts}
\includegraphics[height=6cm]{images/label_counts.png}\label{label_counts}
\end{figure}
\begin{figure}[h!]
\centering
\caption{Distribution of labels}
\includegraphics[height=15cm]{images/label_dist.png}\label{label_dist}
\end{figure}

\subsection{Correlation Analysis}
\subsubsection{Genes}
\subsubsection{Cells}
\subsubsection{Gene-Cells}
In order to investigate the relationship between gene expression and cell viability as a consequence of drug perturbation we computed for each sample the mean gene expression vector \(\mathbf{e}\) and cell viability vector \(\mathbf{v}\) respectively. We then examined pairs of scatter plots with signatures from cytotoxic perturbations (i.e. low cell viability mean) and non-cytotoxic perturbations (i.e high cell viability mean) respectively. We made consistent observations as exemplified in Figure \ref{low_mean} and \ref{high_mean}: An increase in cytotoxicity of a compound  seems to be associated with an increase in the variability of expression levels across genes.

In order to statistically solidify this observation, we  measured the Pearson correlation between the cell viability mean and the standard deviation of the gene expression means and could identify a statistically relevant correlation of -0.8836 (\(p\leq1e-300\)). This suggest that cell death is strongly associated with changes in gene expression.
We further found that the cell viability does not correlate with the gene expression mean (\(p\leq1e-300\)) but, instead, correlates highly with the absolute gene expression mean (\(p\leq1e-300\)) indicating that perturbation induced cell death is not associated with up- or down- regulation of genes specifically but with regulation in general. 

\begin{figure}[h!]
\centering
\caption{Heatmap of Pearson Correlations}
\includegraphics[height=6cm]{images/corr_map.png}\label{corr_map}
\end{figure}

The directionality of the causal relationship between regulation and cell death cannot be established from the correlations alone, but studies suggest that transcription factors may be involved in regulating cell death and profileration and that, hence, the expression signatures may be predictive of the cell viability signatures. Szalai \textit{et al.} studied similar petrurbation-based gene expression and cell viability signatures and identified pathway and transcription factor activities associated with the gene–cell viability correlations using different statistical tools. Some of the most activated (e.g. TP53, FOXO3) and inactivated (e.g. E2F1, FOXM1 and MYC) transcription factors were found to exhibit causal roles as regulators in cell death and proliferation \cite{szalai_signatures_2019}.

These findings bear significance for the inference of mechanism of action from perturbation signatures because the expression of several genes acts as a confounding factor. We shall demonstrate in \ref{moaprediction} that removing some of the genes as features in the model improves the predictive performance.
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  cell\_mean &  cell\_std &  gene\_mean &  gene\_std \\
\midrule
cell\_mean &   1.000000 & -0.729786 &   0.079834 & -0.883603 \\
cell\_std  &  -0.729786 &  1.000000 &  -0.052747 &  0.755585 \\
gene\_mean &   0.079834 & -0.052747 &   1.000000 &  0.010411 \\
gene\_std  &  -0.883603 &  0.755585 &   0.010411 &  1.000000 \\
\bottomrule
\end{tabular}
\caption{Correlations between gene and cell statistics}\label{cell_gene_corr_table}
\end{table}

\begin{figure}[h!]
\centering
\caption{Scatter plot for sample with low cell viability mean}
\includegraphics[height=5cm]{images/low_cell_mean.png}\label{low_mean}
\end{figure}
\begin{figure}[h!]
\centering
\caption{Scatter plot for sample with high cell viability mean}
\includegraphics[height=5cm]{images/high_cell_mean.png}\label{high_mean}
\end{figure}

\subsection{Dimensionality Reduction}

Feature and label correlations
Heatmaps, feature and label distributions
PCA
\clearpage
\section{Model Training}\label{moaprediction}
In this section, we are concerned with learning the functional mapping \[f: \mathbb{R}^{L} \rightarrow \mathbb{R}^{K} \text { that fits }\left\{\mathbf{x}_{i}, \mathbf{y}_{i}\right\}_{i=1}^{N},\] hereby constructing a model that predicts a set of mechanism of actions encoded as a binary vector for a given drug perturbation. 
This task can essentially be described as a \textit{multi-label classification} problem which is a generalization of \textit{multinomial classification} and requires more thought and adaptions in the sampling, learning and evaluation methods than the latter counterpart.\\
\subsection{Data Partitioning}
In order to obtain an generalizable estimate of the performance of our model, we can't compute evaluation metrics on the same dataset as was used for training or optimization as this would result in a biased score that is unlikely to generalize to future, unseen data. In the context of drug action prediction, such a biased score would not give us an accurate assessment of the robustness of our model and the ability to predict the MoA for new unknown compounds. 
We therefore partition our dataset \(\mathscr{D}\) into three disjoint subsets: A training set (70\% of the samples) for fitting the parameters of the model, a validation set (15\% of the samples) for optimizing the hyperparameters which are parameters controlling the learning process and finally a test set (15\% of the samples) on which we evaluate the optimized models in order to provide an approximation of the generalization error.
There exists other data partitioning methods such as \textit{cross-validation} in which a dataset is repeatedly split into a training dataset and a validation dataset that have shown to yield higher average predictive performance that simple splitting \cite{schaffer_selecting_1993}. However, it is still debated whether this method also results in less bias. \\
In our study, the fairly high number of samples reduced the need for cross-validation and our computational restrictions made the expensive cross-validation method rather infeasible. \\
Another requirement for the estimate of generalisation error is that the subsets follow a similar distribution. As show in \ref{data distribution}, the distribution of labels in our dataset is very imbalanced. This is commonly mitigated using stratified sampling in the partition of the datasets. While the implementation of this method for the multinomial classification problem is clear (e.g. groups are differentiated based on the value of the target variable), it is not evident how this would work for our multi-label case. We implement the novel \textit{iterative stratification} algorithm proposed by Sechidis \textit{et al.}. In each iteration it greedily examines the label with the fewest remaining examples in order to avoid that samples may be distributed in an undesired way if rare labels are not examined in priority. For each example of this label, it then selects an appropriate subset for distribution according to empirically validated heuristics \cite{sechidis_stratication_nodate}.
\subsection{Evaluation Measures}
Evaluation measures are critical to the assessment of the performance of a model and there exists trade-offs between different evaluation measures. Its choice can lead to widely diverging interpretations and is therefore of great importance.

In the context of drug discovery and our multi-label classification problem, there are two types of errors that we could make when looking at a specific drug-label pair:
\begin{itemize}
    \item False Positive: Predicting that a drug will have a certain effect that it does.
    \item False Negative: Failing to predict an effect of a drug.
\end{itemize}

Both types of errors are important to the application context of our model because false positives could cause enormous costs if a drug turns out not to be effective after investments in clinical trials have been made, while false negatives can represent a significant opportunity cost if a drug with a rare effect is missed.

Accuracy is the most common evaluation criteria and informally describes the ratio of the number of correct predictions over the total number of predictions:
\[\text { Accuracy }=\frac{T P+T N}{T P+T N+F P+F N}\]

where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.
Accuracy, however, is not well suited to class-imbalanced problems likes ours. A classifier can reach high accuracy scores on imbalanced data by merely predicting the majority class. Accuracy does not distinguish between the numbers of correctly classified examples of different classes \cite{galar_review_2012}.

A better indication the model performance is given by the \textit{receiver operating characteristic} (ROC) which evaluates the system as its discrimination threshold is varied. It plots the True Positive Rate (\(TPR=\frac{TP}{TP+FN}\)) against False Positive Rate ((\(FPR=\frac{FP}{FP+TN}\))) at various threshold values.
The score used for the comparison of models is the area under curve (AUC) of the ROC. AUC provides an aggregate measure of performance across all possible
classification thresholds and can be interpreted as the probability that the model ranks a random positive example more highly than a random negative example.


\subsection{Baselines}
Baselines are critical when approaching a machine learning problem as they guide our further experiments and provide a point of reference from which to compare other algorithms.\\
We implement two different baselines with increasing complexity. The first baseline is a trivial solution and just predicts the most frequent label.
For the second baseline, we chose a simple linear classifier and implemented multinomial regression with a single-layer perceptron with sigmoidal output activation function.
We trained both baselines on the training set and evaluated the models on the testing set. As seen in Table \ref{baseline table}, the simple linear classifier strongly outperforms our trivial baseline.
\begin{table}
\centering
\begin{tabular}{lllll}
\toprule
{Model} &  Train Loss &  Test Loss & Train AUC & Test AUC  \\
\midrule
Majority Label Classifier &  0.0801 & 0.0574 & 0.0052 & 0.0037  \\
Multinomial Regressor  & 0.0128 & 0.0206 & 0.4290 & 0.1075 \\
\bottomrule
\caption{Baseline Results}\label{baseline table}
\end{tabular}

\end{table}
\subsection{Initial Architecture}
Given that the ground-truth values of the model are mechanisms of actions which represent rather abstract concepts, we conjecture that additional hidden layers could be helpful for the learning algorithm to learn more abstract and hereby more discriminative representations. 
We therefore construct an extension of the sigmoidal single-layer perceptron by adding two hidden layers with Rectified Linear Units (ReLus). The first hidden layer maps the input features onto a 2048-dimensional latent space and the second hidden layer maps the latter onto a 1048-dimensional latent space.

\section{Bayesian Optimization}\label{moaprediction}
\section{Model Interpretation}
One of the biggest obstacles in the adoption of deep learning in biomedical applications is its lack of interpretability. Without the interpretabiliy criterion met, physicians and other medical professionals cannot justify taking a decision solely based on the output of a black box algorithm as patients' lives may be at stake. In the context of our work, understanding why a compound was attributed a certain mechanism of action and which genes, pathways and biological processes are led to the classification is critical in order to be able to move forward with the costly and sensitive drug development process. Interpretability further ensures that a neural network bases its predictions on reliable representations and signals of the data. Last but not least, we are interested in the patterns identified by the neural network and whether they are able to generate new biological insights.

\subsection{Integrated Gradients}
In this section we are interested in why a specific compound was assigned a particular mechanism of action and how it achieves that particular MoA.
For instance, the MoA of anti-inflammatory drugs such as Aspirin and Ibuprofen is \textbf{cyclooxygenase inhibitor} which describes the inhibition of cyclooxygenase enzymes which cause inflammation and pain. However, the way Ibuprofen and Aspirin achieve the inhibition of these enzymes can involve completely different pathways and biological processes. Identifying these can help us assess the efficacy, safety and side effects of the drug.

Recent works in methods for interpreting deep neural networks show that gradient methods produce better interpretations of a neural network than simple weight analysis \cite{montavon_methods_2018}. Backpropagating the activation of the output neuron and analysing the product of the gradient and feature values is a reasonable start for feature attribution

The principle of gradient methods is to backpropagate the activation of the output neuron through the network and to estimate for each layer the impact of the neurons and the connections on the output
We tackle this problem by determining the genes and cells that most contributed to the classification in the the neural network and then enrich these features with annotations from biological process and pathway databases. 


By identifying the genes and cells that most contributed to the MoA classification, we can infer which biological processes achieve that MoA and whether we can i

This question is of interest because it  I.e. we want to identify the biological processes

\section{Causal Inference}

\chapter{Conclusions}

\chapter{Future Work}
Include domain specific data

Limitations
CMap comes with some limitations, such as limited drug perturbation data, a limited drug coverage, dosage-dependent conditions and the uncertainty of applying cell lines or animal model expression patterns to human systems. Also, the methodology can be expensive and time-consuming before it can generate a significant portion of all safe dosage conditions for a limited number of cell lines for CMap

% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}
